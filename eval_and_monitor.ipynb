{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data Generated from the Previous Module\n",
    "\n",
    "This dataset, derived from the gpt4o mini, includes fields such as 'answer_llm', 'answer_orig', 'document', 'question', and 'course'.\n",
    "\n",
    "- 'answer_llm': The response provided by the language model to the generated question.\n",
    "- 'answer_orig': The original answer from the ground truth dataset, which was used to generate the corresponding question.\n",
    "- 'document': The identifier for the document.\n",
    "- 'question': The question generated from the original answer.\n",
    "- 'course': The related course.\n",
    "\n",
    "The dataset was compiled by processing a ground truth dataset that contains questions, associated courses, and document IDs. This ground truth dataset was produced from the original set of FAQ documents. For each answer in these documents, an LLM generated five related questions, recording the document ID linked to each question.\n",
    "\n",
    "Using the fields 'answer_llm' and 'answer_orig', we can compute the cosine similarity to assess the accuracy of the LLM's responses to the generated questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv'\n",
    "url = f'{github_url}?raw=1'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:300] #only use the first 300 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the embeddings model\n",
    "\n",
    "multi-qa-mpnet-base-dot-v1 from the Sentence Transformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/share/virtualenvs/llm-homework-04-4mt2ev0V/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = 'multi-qa-mpnet-base-dot-v1'\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embeddings for the first LLM answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).\n"
     ]
    }
   ],
   "source": [
    "answer_llm = df.iloc[0].answer_llm\n",
    "print(answer_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.42244658\n"
     ]
    }
   ],
   "source": [
    "first_answer_embedding = embedding_model.encode(answer_llm)\n",
    "print(first_answer_embedding[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Dot Product\n",
    "\n",
    "Now for each answer pair, let's create embeddings and compute dot product between them\n",
    "\n",
    "We will put the results (scores) into the `evaluations` list\n",
    "\n",
    "What's the 75% percentile of the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:04<00:00,  4.64it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    v_answer_llm = embedding_model.encode(row['answer_llm']) \n",
    "    v_answer_orig = embedding_model.encode(row['answer_orig'])\n",
    "    score = v_answer_llm.dot(v_answer_orig)\n",
    "    evaluations.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 75th percentile is: 31.674313\n"
     ]
    }
   ],
   "source": [
    "percentile_75 = np.percentile(evaluations, 75)\n",
    "\n",
    "print(\"The 75th percentile is:\", percentile_75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The results are not within the range [0,1]. We need to normalize the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_norm(v):\n",
    "    norm = np.sqrt((v * v).sum())\n",
    "    v_norm = v / norm\n",
    "    return v_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df):\n",
    "    evaluations = []\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        v_answer_llm = compute_norm(embedding_model.encode(row['answer_llm'])) \n",
    "        v_answer_orig = compute_norm(embedding_model.encode(row['answer_orig']))\n",
    "        score = v_answer_llm.dot(v_answer_orig)\n",
    "        evaluations.append(score)\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:04<00:00,  4.64it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluations = evaluate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 75th percentile is: 0.8362348\n"
     ]
    }
   ],
   "source": [
    "percentile_75 = np.percentile(evaluations, 75)\n",
    "\n",
    "print(\"The 75th percentile is:\", percentile_75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will use an alternative metric - the ROUGE score\n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (doc_id=5170565b)\n",
    "\n",
    "n-grams are contiguous sequence of n items from a given sample of text or speech. These items can be phonemes, syllables, letters, words, or base pairs according to the level of textual analysis being conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three scores: rouge-1, rouge-2 and rouge-l, and precision, recall and F1 score for each.\n",
    "\n",
    "- rouge-1 - the overlap of unigrams,\n",
    "- rouge-2 - bigrams,\n",
    "- rouge-l - the longest common subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.45454545454545453,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.45454544954545456},\n",
       " 'rouge-2': {'r': 0.21621621621621623,\n",
       "  'p': 0.21621621621621623,\n",
       "  'f': 0.21621621121621637},\n",
       " 'rouge-l': {'r': 0.3939393939393939,\n",
       "  'p': 0.3939393939393939,\n",
       "  'f': 0.393939388939394}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average rouge score\n",
    "\n",
    "Let's compute the average F-score between rouge-1, rouge-2 and rouge-l for the same record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average F-score is: 0.35490034990035496\n"
     ]
    }
   ],
   "source": [
    "# Extract F-scores using list comprehension\n",
    "f_scores = [scores[key]['f'] for key in scores]\n",
    "\n",
    "# Calculate the average F-score using NumPy\n",
    "average_f_score = np.mean(f_scores)\n",
    "\n",
    "print(\"The average F-score is:\", average_f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average rouge score for all the data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the F-score for all the records and create a dataframe from them.\n",
    "\n",
    "What's the average F-score in rouge_2 across all the records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with predefined columns\n",
    "f_scores_df = pd.DataFrame(columns=['rouge-1_f', 'rouge-2_f', 'rouge-l_f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:01<00:00, 262.99it/s]\n"
     ]
    }
   ],
   "source": [
    "rows_list = []  # List to collect each row dictionary\n",
    "\n",
    "for index, r in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    # Compute scores\n",
    "    scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "    \n",
    "    # Extract and store F-scores\n",
    "    f_score_record = {\n",
    "        'rouge-1_f': scores['rouge-1']['f'],\n",
    "        'rouge-2_f': scores['rouge-2']['f'],\n",
    "        'rouge-l_f': scores['rouge-l']['f']\n",
    "    }\n",
    "    \n",
    "    # Append the record to the list\n",
    "    rows_list.append(f_score_record)\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "f_scores_df = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average F-score for ROUGE-2 across all records is: 0.20696501983423318\n"
     ]
    }
   ],
   "source": [
    "# Now, calculate the average F-score for 'rouge-2'\n",
    "average_rouge_2_f = f_scores_df['rouge-2_f'].mean()\n",
    "print(\"The average F-score for ROUGE-2 across all records is:\", average_rouge_2_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-homework-04-4mt2ev0V",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
